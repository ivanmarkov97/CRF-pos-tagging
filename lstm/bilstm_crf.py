# -*- coding: utf-8 -*-
"""BiLSTM-CRF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HDsX4cW2ywk_gfR4edl7G9yjHsyqu6mV
"""

#!pip install --quiet pytorch-crf

import os
import random
import numpy as np

import functools

import torch
import torch.nn as nn

from torchtext import datasets
from torchtext.data import Field
from torchtext.data import BucketIterator

from torchcrf import CRF


SEED = 241

def seed_everything(seed):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  os.environ['PYTHONHASHSEED'] = str(seed)

  if torch.cuda.is_available(): 
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(SEED)

TEXT = Field(lower=True,
             use_vocab=True,
             sequential=True,
             batch_first=True,
             include_lengths=True)

LABEL = Field(lower=True,
              use_vocab=True,
              sequential=True,
              unk_token = None,
              batch_first=True)

fields = [('text', TEXT), ('tags', LABEL)]

train_data, valid_data, test_data = datasets.UDPOS.splits(fields)

TEXT.build_vocab(train_data,
                 max_size=25000,
                 vectors='glove.6B.100d')
LABEL.build_vocab(train_data)

batch_size = 32
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')


train_iterator = BucketIterator.splits((train_data,), batch_size=batch_size, device=device)[0]
valid_iterator = BucketIterator.splits((valid_data,), batch_size=batch_size, device=device)[0]
test_iterator = BucketIterator.splits((test_data,), batch_size=batch_size, device=device)[0]

class BiLSTM_CRF_Tagger(nn.Module):

  def __init__(self, vocab_size, emb_size, hidden_size, n_layers, dropout, num_tags, pad_idx):
    super().__init__()
    self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)
    self.rnn = nn.LSTM(emb_size,
                       hidden_size, 
                       num_layers=n_layers,
                       dropout=0.3, 
                       bidirectional=True, 
                       batch_first=True)
    
    self.dropout = nn.Dropout(dropout)
    self.hidden2tag = nn.Linear(emb_size, num_tags)
    self.crf = CRF(num_tags, batch_first=True)

  def _generate_mask(self, text_lens):
    bs = text_lens.size(0)
    max_seq_len = torch.max(text_lens).item()
    mask = torch.ByteTensor(bs, max_seq_len).fill_(0)
    for i in range(bs):
      mask[i, :text_lens[i]] = 1
    return mask

  def forward(self, text, text_lens, tags=None):
    text_embed = self.embedding(text)

    text_packed = nn.utils.rnn.pack_padded_sequence(text_embed, text_lens, batch_first=True, enforce_sorted=False)
    rnn_outputs, (last_hidden, cell_state) = self.rnn(text_packed)
    text_unpacked, lens_unpacked = nn.utils.rnn.pad_packed_sequence(text_packed, batch_first=True)
    last_hidden = last_hidden.permute(1, 0, 2)

    emission = self.hidden2tag(text_unpacked)
    mask = self._generate_mask(text_lens).to(device)

    if tags is not None:
      loss = -self.crf.forward(torch.log_softmax(emission, dim=2), tags, mask, reduction='mean')
      return loss
    else:
      prediction = self.crf.decode(emission, mask)
      return prediction

VOCAB_SIZE = len(TEXT.vocab)
EMB_SIZE = 100
HIDDEN_SIZE = 128
N_LAYERS = 2
DROPOUT = 0.3
NUM_TAGS = len(LABEL.vocab)
PAD_IDX = LABEL.vocab.stoi['<pad>']


model = BiLSTM_CRF_Tagger(VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT, NUM_TAGS, PAD_IDX)
optimizer = torch.optim.Adam(model.parameters())

model.embedding.weight.data.copy_(TEXT.vocab.vectors)

model = model.to(device)

from tqdm import tqdm


for i in range(10):
  model.train()
  error = 0.

  for batch in tqdm(train_iterator):
    optimizer.zero_grad()

    text, lens = batch.text
    lens = lens.cpu()
    tags = batch.tags
    
    loss = model(text, lens, tags)
    loss.backward()

    optimizer.step()
    error += loss.detach().cpu().numpy()
  print('train error', error / len(train_iterator))

  error = 0.
  model.eval()
  with torch.no_grad():
    for batch in tqdm(valid_iterator):
      text, lens = batch.text
      lens = lens.cpu()
      tags = batch.tags
      
      loss = model(text, lens, tags)
      error += loss.detach().cpu().numpy()
    print('valid error', error / len(valid_iterator))

def calculate_accuracy(y_true, y_pred):
  assert y_true.shape == y_pred.shape
  assert len(y_true.shape) == 1
  return (y_true == y_pred).sum() / y_true.shape[0]

total_true_labels = []
total_pred_labels = []

for index in range(len(test_data.examples)):

  text = test_data.examples[index].text
  true_labels = test_data.examples[index].tags

  with torch.no_grad():
    tokens = text
    ids = [TEXT.vocab.stoi[token] for token in tokens]
    ids_tensor = torch.tensor([ids], device=device)
    lens = torch.tensor([len(ids)])
    prediction = model(ids_tensor, lens)
    
  print('\t'.join(tokens))
  print('\t'.join(true_labels))
  print('\t'.join([LABEL.vocab.itos[p] for p in prediction[0]]))
  print('='*20)

  total_true_labels.extend(np.array([LABEL.vocab.itos[p] for p in prediction[0]]))
  total_pred_labels.extend(np.array(true_labels))

print(calculate_accuracy(np.array(total_true_labels), np.array(total_pred_labels)))

from sklearn.metrics import classification_report


print(classification_report(np.array(total_true_labels), np.array(total_pred_labels)))

