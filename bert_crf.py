# -*- coding: utf-8 -*-
"""BERT-CRF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QYFhPf6OIkyL0agajg9VFicaKgr0msir
"""

# !pip3 install --quiet transformers
# !pip3 install --quiet pytorch-crf

import os
import random
import numpy as np

import functools

import torch
import torch.nn as nn

from torchtext import datasets
from torchtext.data import Field
from torchtext.data import BucketIterator

from transformers import BertTokenizer, BertModel


BERT_VERSION = 'bert-base-uncased'

SEED = 241

def seed_everything(seed_value):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    
    if torch.cuda.is_available(): 
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = True

seed_everything(SEED)

bert = BertModel.from_pretrained(BERT_VERSION, output_hidden_states=True)
tokenizer = BertTokenizer.from_pretrained(BERT_VERSION)

tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id

def tokenize_and_convert_text(tokens, tokenizer, max_len):
  tokens = tokens[:max_len]
  token_ids = tokenizer.convert_tokens_to_ids(tokens)
  return token_ids

def tokenize_and_convert_labels(labels, max_len):
  return labels[:max_len]


text_preprocess = functools.partial(tokenize_and_convert_text, 
                                    tokenizer=tokenizer, 
                                    max_len=tokenizer.max_len_single_sentence)
tag_preprocess = functools.partial(tokenize_and_convert_labels,
                                   max_len=tokenizer.max_len_single_sentence)

TEXT = Field(lower=True,
             sequential=True,
             use_vocab=False,
             batch_first=True,
             include_lengths=True,
             preprocessing=text_preprocess,
             init_token=tokenizer.cls_token_id,
             pad_token=tokenizer.pad_token_id,
             unk_token=tokenizer.unk_token_id)
             # eos_token=tokenizer.sep_token_id)

LABEL = Field(batch_first=True,
              unk_token = None,
              init_token='<pad>',
              preprocessing=tag_preprocess)

fields = [('text', TEXT), ('udtags', LABEL)]

train_data, valid_data, test_data = datasets.UDPOS.splits(fields=fields)

LABEL.build_vocab(train_data)

batch_size = 32
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator = BucketIterator.splits((train_data, ),
                                       batch_size=batch_size,
                                       device=device)[0]

valid_iterator = BucketIterator.splits((valid_data, ),
                                       batch_size=batch_size,
                                       device=device)[0]

test_iterator = BucketIterator.splits((test_data, ),
                                      batch_size=batch_size,
                                      device=device)[0]

from torchcrf import CRF


class BertCRFTagger(nn.Module):

  def __init__(self, bert, hidden_size, num_tags, dropout):
    super().__init__()
    self.bert = bert
    self.crf = CRF(num_tags, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_tags)
    self.dropout = nn.Dropout(dropout)

  def generate_mask(self, input_temaplte):
    bs = input_temaplte.size(0)
    seq_len = torch.max(input_temaplte)
    mask = torch.ByteTensor(bs, seq_len).fill_(0)
    for i in range(bs):
      mask[i, :input_temaplte[i]] = 1
    return mask

  def forward(self, input_ids, text_lens, tags=None):
    bert_output = self.bert(input_ids)
    last_hidden_state = bert_output['hidden_states'][-1]
    
    emission = self.fc(last_hidden_state)
    mask = self.generate_mask(text_lens).to(device)

    if tags is not None:
      loss = -self.crf(torch.log_softmax(emission, dim=2), tags, mask=mask, reduction='mean')
      return loss
    else:
      prediction = self.crf.decode(emission, mask=mask)
      return prediction

dropout = 0.3
num_tags = len(LABEL.vocab.itos)
hidden_size = bert.config.to_dict()['hidden_size']


bert_crf_tagger = BertCRFTagger(bert, hidden_size, num_tags, dropout).to(device)
optimizer = torch.optim.Adam(bert_crf_tagger.parameters(), lr=2e-5)

from tqdm import tqdm

for i in range(2):
  bert_crf_tagger.train()
  error = 0.
  for batch in tqdm(train_iterator):
      optimizer.zero_grad()

      text, lens = batch.text
      labels = batch.udtags

      loss = bert_crf_tagger(text, lens, labels)

      loss.backward()
      optimizer.step()

      error += loss.detach().cpu().item()
  print('train error', error / len(train_iterator))

  error = 0.
  bert_crf_tagger.eval()
  with torch.no_grad():
    for batch in tqdm(valid_iterator):

        text, lens = batch.text
        labels = batch.udtags
        loss = bert_crf_tagger(text, lens, labels)

        error += loss.detach().cpu().item()
  print('valid error', error / len(valid_iterator))

import numpy as np


def calculate_accuracy(y_true, y_pred):
  assert y_true.shape == y_pred.shape
  assert len(y_true.shape) == 1
  y_true = y_true[1:]
  y_pred = y_pred[1:]
  return (y_true == y_pred).sum() / y_true.shape[0]

total_true_labels = []
total_pred_labels = []

for index in range(len(test_data.examples)):

  text = '\t'.join(tokenizer.convert_ids_to_tokens(test_data.examples[index].text))
  true_labels = '\t'.join(['<pad>'] + test_data.examples[index].udtags)

  with torch.no_grad():
    tokens = ['[CLS]'] + text.split()[:tokenizer.max_len_single_sentence]
    ids = tokenizer.convert_tokens_to_ids(tokens)
    ids_tensor = torch.tensor([ids], device=device)
    lens = torch.tensor([len(ids)]).to(device)
    prediction = bert_crf_tagger(ids_tensor, lens)
    
  print('\t'.join(tokens))
  print(true_labels)
  print('\t'.join([LABEL.vocab.itos[p] for p in prediction[0]]))

  total_true_labels.extend(np.array([LABEL.vocab.itos[p] for p in prediction[0]]))
  total_pred_labels.extend(np.array(true_labels.split('\t')))

print(calculate_accuracy(np.array(total_true_labels), np.array(total_pred_labels)))

from sklearn.metrics import classification_report


print(classification_report(np.array(total_true_labels), np.array(total_pred_labels)))

